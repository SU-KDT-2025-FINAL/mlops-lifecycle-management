1. 데이터 생성 및 파생 변수 추가
먼저 고객 데이터를 생성하고, 예측에 필요한 파생 변수를 추가합니다.

```python
import pandas as pd
import numpy as np

# 샘플 데이터 생성 및 파생 변수 추가


np.random.seed(42)

def generate_customer_data(n_samples=1000):
    data = {
        '사용기간': np.random.randint(1, 60, n_samples),  # 고객 사용 기간 (개월)
        '구매빈도': np.random.randint(1, 20, n_samples),  # 고객의 구매 빈도
        '평균구매금액': np.random.normal(50000, 20000, n_samples),  # 고객의 평균 구매 금액
        '문의횟수': np.random.randint(0, 10, n_samples),  # 고객의 서비스 문의 횟수
    }
    df = pd.DataFrame(data)

    # 파생 변수 생성
    df['월평균구매금액'] = df['평균구매금액'] / df['사용기간']
    df['사용기간_구매빈도'] = df['사용기간'] / (df['구매빈도'] + 1)  # 0 방지

    # 이탈 여부 생성: 예시 조건에 맞는 고객을 이탈한 고객으로 지정
    df['이탈여부'] = (
        (df['사용기간'] < 12) & 
        (df['구매빈도'] < 5) & 
        (df['평균구매금액'] < 30000) | 
        (df['문의횟수'] > 7)
    ).astype(int)
    return df

df = generate_customer_data()
```

2. 데이터 전처리


데이터를 학습용과 테스트용으로 나누고, 스케일링을 적용합니다.
```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


# 데이터 분할 및 스케일링

```python
X = df[['사용기간', '구매빈도', '평균구매금액', '문의횟수', '월평균구매금액', '사용기간_구매빈도']]
y = df['이탈여부']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

3. 특징 중요도 분석
랜덤포레스트를 사용하여 각 특징의 중요도를 분석하고, 이를 시각화합니다.

```python
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# 랜덤포레스트로 특징 중요도 추출
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train_scaled, y_train)

# 중요도 시각화
importances = rf.feature_importances_
features = X.columns
importance_df = pd.DataFrame({'특징': features, '중요도': importances}).sort_values(by='중요도', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='중요도', y='특징', data=importance_df, palette='viridis')
plt.title('특징 중요도 분석')
plt.show()
```

4. 모델 학습 및 성능 평가
다양한 모델(로지스틱 회귀, 의사결정나무, 랜덤포레스트)을 학습시키고 성능을 평가합니다.

```python
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix

# 모델 학습 및 평가 함수
def evaluate_model(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    print(f"{type(model).__name__} 성능 평가:")
    print(classification_report(y_test, y_pred))
    
    # 혼동행렬 시각화
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'{type(model).__name__} 혼동행렬')
    plt.ylabel('실제 값')
    plt.xlabel('예측 값')
    plt.tight_layout()
    plt.show()

# 모델 리스트
models = [
    LogisticRegression(),
    DecisionTreeClassifier(random_state=42),
    RandomForestClassifier(random_state=42)
]

for model in models:
    evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test)
```

| 단계                | 주요 내용                                          | 상세 설명/사용 기법                                |
|---------------------|---------------------------------------------------|---------------------------------------------------|
| 1. 데이터 생성 및 파생변수 | - 샘플 고객 데이터 생성<br>- 파생 변수 추가       | - Pandas, NumPy 사용<br>- 월평균 구매금액, 사용기간/구매빈도 등 파생 변수 생성 |
| 2. 데이터 전처리     | - 학습/테스트 데이터 분할<br>- 스케일링 적용         | - `train_test_split`<br>- `StandardScaler`로 정규화 |
| 3. 특징 중요도 분석  | - 랜덤포레스트로 변수 중요도 평가<br>- 시각화         | - `RandomForestClassifier`<br>- 시각화: matplotlib, seaborn |
| 4. 모델 학습 및 평가 | - 여러 분류 모델로 이탈 예측<br>- 성능 비교 및 해석    | - 로지스틱 회귀<br>- 의사결정나무<br>- 랜덤포레스트<br>- 성능지표(classification_report, confusion_matrix), 혼동행렬 시각화 |

##  ^^^^Random Forest 모델이 가장 높은 정확도와 안정적인 성능^^^^